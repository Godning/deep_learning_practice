{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ref and Codes: https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook\n",
    "- Ref Git: https://github.com/iamaaditya/VQA_Keras\n",
    "- Ref Git: https://github.com/iamaaditya/VQA_Demo\n",
    "- **Copied and slightly modified from the repos above for self-learning purpose**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/Za5P1ZZ.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import os, argparse\n",
    "import cv2, spacy, numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D as Convolution2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with input (embedding question vector, extracted image 4096 vector), and output [0,1000] classification\n",
    "VQA_model_file_name      = './model/VQA/VQA_MODEL.json' \n",
    "VQA_weights_file_name   = './model/VQA/VQA_MODEL_WEIGHTS.hdf5'\n",
    "# convert from [0,1000] to label text\n",
    "label_encoder_file_name  = './model/VQA/FULL_labelencoder_trainval.pkl'\n",
    "# model with input (224 * 224) image and output 4096 feature vector\n",
    "CNN_weights_file_name   = './model/VQA/vgg16_weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop one model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop(model):\n",
    "    '''Removes a layer instance on top of the layer stack.\n",
    "    This code is thanks to @joelthchao https://github.com/fchollet/keras/issues/2371#issuecomment-211734276\n",
    "    '''\n",
    "    if not model.outputs:\n",
    "        raise Exception('Sequential model cannot be popped: model is empty.')\n",
    "    else:\n",
    "        model.layers.pop()\n",
    "        if not model.layers:\n",
    "            model.outputs = []\n",
    "            model.inbound_nodes = []\n",
    "            model.outbound_nodes = []\n",
    "        else:\n",
    "            model.layers[-1].outbound_nodes = []\n",
    "            model.outputs = [model.layers[-1].output]\n",
    "        model.built = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_legacy(model, weight_path):\n",
    "    ''' this function is used because the weights in this model\n",
    "    were trained with legacy keras. New keras does not support loading these weights '''\n",
    "\n",
    "    import h5py\n",
    "    f = h5py.File(weight_path, mode='r')\n",
    "    flattened_layers = model.layers\n",
    "\n",
    "    nb_layers = f.attrs['nb_layers']\n",
    "\n",
    "    for k in range(nb_layers):\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        if not weights: continue\n",
    "        if len(weights[0].shape) >2: \n",
    "            # swap conv axes\n",
    "            # note np.rollaxis does not work with HDF5 Dataset array\n",
    "            weights[0] = np.swapaxes(weights[0],0,3)\n",
    "            weights[0] = np.swapaxes(weights[0],0,2)\n",
    "            weights[0] = np.swapaxes(weights[0],1,2)\n",
    "        flattened_layers[k].set_weights(weights)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64,( 3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128,( 3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256,( 3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512,( 3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    if weights_path:\n",
    "        # model.load_weights(weights_path)\n",
    "        load_model_legacy(model, weights_path)\n",
    "\n",
    "    #Remove the last two layers to get the 4096D activations\n",
    "    model = pop(model)\n",
    "    model = pop(model)\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw image features as `VGG` input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract/resize raw image features with `cv2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_features = np.zeros((1, 4096))\n",
    "    # Magic_Number = 4096  > Comes from last layer of VGG Model\n",
    "\n",
    "    # Since VGG was trained as a image of 224x224, every new image\n",
    "    # is required to go through the same transformation\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n",
    "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
    "\n",
    "    \n",
    "    # this axis dimension is required because VGG was trained on a dimension\n",
    "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
    "    # even though we are using only one image, we have to keep the dimensions consistent\n",
    "    im = np.expand_dims(im, axis=0) \n",
    "\n",
    "    image_features[0,:] = model_vgg.predict(im)[0]\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed question text with pretrained word vectors as `VQA` input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_features(question):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    word_embeddings = spacy.load('en_vectors_web_lg')\n",
    "    tokens = word_embeddings(question)\n",
    "    question_tensor = np.zeros((1, 30, 300))\n",
    "    for j in range(len(tokens)):\n",
    "        question_tensor[0,j,:] = tokens[j].vector\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get VGG model with loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model(CNN_weights_file_name):\n",
    "    image_model = VGG_16()\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "model_vgg = get_image_model(CNN_weights_file_name)\n",
    "plot_model(model_vgg, to_file='model_vgg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get VQA model with loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VQA_model(VQA_model_file_name, VQA_weights_file_name):\n",
    "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
    "\n",
    "    # thanks the keras function for loading a model from JSON, this becomes\n",
    "    # very easy to understand and work. Alternative would be to load model\n",
    "    # from binary like cPickle but then model would be obfuscated to users\n",
    "    vqa_model = model_from_json(open(VQA_model_file_name).read())\n",
    "    vqa_model.load_weights(VQA_weights_file_name)\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "model_vqa  = get_VQA_model(VQA_model_file_name, VQA_weights_file_name)\n",
    "plot_model(model_vqa, to_file='model_vqa.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_name = './fig/test.jpg'\n",
    "question = u\"What vehicle is in the picture?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/test.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the raw image features\n",
    "image_features = get_image_features(image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the raw question features\n",
    "question_features = get_question_features(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.16 %  piano\n",
      "17.09 %  train\n",
      "15.23 %  bus\n",
      "011.1 %  bicycle\n",
      "09.82 %  airplane\n"
     ]
    }
   ],
   "source": [
    "y_output = model_vqa.predict([question_features, image_features])\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# This task here is represented as a classification into a 1000 top answers\n",
    "# this means some of the answers were not part of training and thus would \n",
    "# not show up in the result.\n",
    "# These 1000 answers are stored in the sklearn Encoder class\n",
    "labelencoder = joblib.load(label_encoder_file_name)\n",
    "for label in reversed(np.argsort(y_output)[0,-5:]):\n",
    "    print (str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
