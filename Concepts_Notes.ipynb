{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "- Batch (Fast Convergence)\n",
    "- SGD\n",
    "\n",
    "**Back-propagation**\n",
    "- Preprocessing (e.g., zero-centered)\n",
    "\n",
    "**Activation Function**\n",
    "- Sigmoid\n",
    "    * Saturated at 0/1 and kills gradients (derivative -> 0)\n",
    "    * Output not zero-centred; for next layer: f = wx + b, x>0, df/dw same sign for all w; zig-zag update trajectory\n",
    "- Tanh\n",
    "    * Still kills gradients\n",
    "    * But: zero-cented\n",
    "- ReLU\n",
    "    * Non-saturated, linearity --> Accelerate convergence\n",
    "    * Cheap computation\n",
    "    * But: Can die; never activate\n",
    "    * Extension: Leaky ReLU, maxout\n",
    "    \n",
    "**NN as universal approximators**\n",
    "- More neurons --> more complicated functions\n",
    "- Regularization to prevent overfitting\n",
    "\n",
    "**Regularization**\n",
    "- L1/L2/ElasticNet\n",
    "- Max Norm constraint\n",
    "- Drop Out layer\n",
    "\n",
    "**Hyperparameter Optimization**\n",
    "- Single validation set > cross validation in practice\n",
    "- Random search instead of grid search within a range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvNets**\n",
    "- Difference with regular NN:\n",
    "    * Main difference: each neuron is layer 2 is only connected to a few neurons in layer 1\n",
    "    * Data arranged in 3 dimensions: height, width, and depth\n",
    "- Convolutional Layer:\n",
    "    * Filter (with full depth, but local connectivity across 2d), 3\\*3 --> 5\\*5\n",
    "    * The depth of layer 2 == The number of filters in Layer 1\n",
    "    * `Stride`: usually 1, leaving downsampling to pooling layer. Can use 2 to compromise 1st layer because of computational constraints\n",
    "    * `Padding`: use same to avoid missing information along the border\n",
    "    * *Parameter Sharing*: Same weight for filter/kernel at same depth slice in layer 2 (Alternative: local)\n",
    "- Pooling\n",
    "    * Most commonly: 2-2 Max Pooling\n",
    "- Fully-Connected Layer\n",
    "- Common architecture:\n",
    "    * INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]\\* -> [FC -> RELU]\\*2 -> FC\n",
    "    * Prefer a stack of small filter CONV to one large receptive field CONV layer\n",
    "- Challenge: Computational resources\n",
    "\n",
    "\n",
    "**Transfer Learning**\n",
    "- Apply trained model without last FC layer and use it as feature extracter\n",
    "- Continue to fine tune the model using smaller learning rate\n",
    "- Can use different image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights Initialization**\n",
    "- **All zero**: wrong: neuron outputs and gradients would be same; same update\n",
    "- **Number to small**: small gradients for inputs; gradient diminishing when flowing backwafrd\n",
    "- **Preferred: All neuron with same output distribution**: \n",
    "    - w = np.random.randn(n) / sqrt(n), where n is number of inputs. \n",
    "    - It can be proved that Var(S) = Var(WX) = Var(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Param Update and Learning Rate**\n",
    "- Step decay for learning rate: \n",
    "    * Reduce the learning rate by some factor every few epochs. \n",
    "    * Other approaches also avalable, like exponential decay, 1/t decay, etc.\n",
    "- Second-order update method:\n",
    "    * i.e., Newton's method, not common\n",
    "- Per-parameter adaptive learning rate methods: \n",
    "    * For example: Adagrad, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Workflow**\n",
    "- Preprocess Data (zero-centered,i.e.,Substractmean)\n",
    "- Identify architecture\n",
    "- Ensure that we can overfit a small traing set to acc = 100%\n",
    "- Loss not decreasing: too low leaning rate\n",
    "- Loss goes to NaN: too high learning rate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization**\n",
    "\n",
    "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "\n",
    "- Improve gradient flow\n",
    "- Allow higher learning rates\n",
    "- Reduce dependence on initialization\n",
    "- Some regularization\n",
    "- *Note*: At test time, the mean from training should be used instead of calculated from testing batch\n",
    "\n",
    "- Fully connected layer\n",
    "    * X: (num_example, dimension)\n",
    "    * mu, sigma: 1 \\* D\n",
    "\n",
    "\n",
    "- CNN -> Spatial Batchnorm\n",
    "    * X: (num_example, channels, height, width)\n",
    "    * mu, sigma: 1 \\* C \\* 1 \\* 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization**\n",
    "- Fully connected layer\n",
    "    * X: (num_example, dimension)\n",
    "    * mu, sigma: N \\* 1 \n",
    "    * Note: same behavior during testing\n",
    "\n",
    "**Instance Normalization**\n",
    "- CNN\n",
    "    * X: (num_example, channels, height, width)\n",
    "    * mu, sigma: N \\* C \\* 1 \\* 1\n",
    "    \n",
    "**Second Order Optimization**\n",
    "- No Hyperparameter and learning rates\n",
    "- N^2 elements, O(N^3) for taking inverting\n",
    "- Methods:\n",
    "    * Quasi-Newton methods(BGFS): O(N^3) -> O(N^2)\n",
    "    * L-BFGS: Does not form/store the full inverse Hessian.\n",
    "    \n",
    "**Hardware**\n",
    "- CPU: less cores, faster per core, better at sequential tasks\n",
    "- GPU: more cores, slower per core, better at parallel tasks\n",
    "- TPU: just for DL (Tensor Processing Unit)\n",
    "    * Split *One* graph over *Multiple* machines\n",
    "\n",
    "**Software**\n",
    "- Caffe (FB)\n",
    "- PyTorch (FB)\n",
    "- TF (Google)\n",
    "- CNTK (MS)\n",
    "- Dynamic (e.g., Eager Execution) vs. Static (e.g., TF Lower-level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "- LeNet-5 (CONV-POOL-CONV-POOL-FC-FC)\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/tLKYz.png\" width=\"800\">\n",
    "\n",
    "- AlexNet 8 layers (CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8)\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_1.jpg\" width=\"400\">\n",
    "\n",
    "- ZFNet (similar with AlexNet)\n",
    "    * Smaller kernel, more filters\n",
    "\n",
    "\n",
    "- VGGNet(*smaller* filter and *deeper* networks)\n",
    "    * 16-19 layers in VGG16Net\n",
    "    * Three 3 \\* 3 kernel with stride == One 7 \\* 7 kernel; Same *effective receptive field\n",
    "    * But: 1) deeper network and more non-linearity; 2) less parameters ( 3 \\* 3 \\* 3 vs. 7 \\* 7)\n",
    "<img src=\"http://josephpcohen.com/w/wp-content/uploads/Screen-Shot-2016-01-14-at-11.25.15-AM.png\" width=\"800\">\n",
    "\n",
    "\n",
    "- GoogLeNet\n",
    "    * Introduced *inception* Module (Parallel filter operations with multiple kernel size)\n",
    "    * Problem: Output size too big after filter concatenation\n",
    "    * The purpose of 1 \\* 1 convolutonal layer: \n",
    "        - Pooling layer keeps the same depth as input\n",
    "        - 1 \\* 1 layer keeps the same dimension of input, and reduces depth (for example: 64 \\* 56 \\* 56 after 32 1 \\* 1 con --> 32 \\* 56 \\* 56)\n",
    "        - Reduce total number of operations\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Bo_Zhao48/publication/312515254/figure/fig3/AS:489373281067012@1493687090916/nception-module-of-GoogLeNet-This-figure-is-from-the-original-paper-10.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "- ResNet\n",
    "    * Use network layers to fit a *Residual mapping* instead of directly fitting a desired underlying mapping\n",
    "    * Residual blocks are stacked\n",
    "    * Similar to GoogLeNet, can use *bottelneck* layer (1 \\* 1 conv layer) for downsampling and efficiency ++\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Antonio_Theophilo/publication/321347448/figure/fig2/AS:565869411815424@1511925189281/Bottleneck-Blocks-for-ResNet-50-left-identity-shortcut-right-projection-shortcut.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "- What is the problem with RNN\n",
    "    * Gradient Vanishing/Exploding with Vanilla RNN\n",
    "    * Computing gradient of $ h_0 $ involved many multiplication of **W** and **tanh** activation\n",
    "    * Brief proof:\n",
    "    \n",
    "    $ \\frac{ \\partial E_t} { \\partial w}\n",
    "    = \\sum_{k=1}^{t}  \\frac{ \\partial E_t} { \\partial y_t}\n",
    "                      \\frac{ \\partial y_t} { \\partial h_t}\n",
    "                      \\frac{ \\partial h_t} { \\partial h_k}\n",
    "                      \\frac{ \\partial h_k} { \\partial w}$\n",
    "    Here:$ h_t = W_{hh} f(h_{t-1}) + W_{hx} X_t$              \n",
    "    $ \\frac{ \\partial h_t} { \\partial h_k}\n",
    "    = \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}} $\n",
    "    \n",
    "    $ \\| \\frac{ \\partial h_t} { \\partial h_k} \\|\n",
    "    \\leq (\\beta_W \\beta_h)^{t-k} $ $\\beta$ is upper bound for matrix norms\n",
    "    \n",
    "    $ \\| W^T_{hh} \\| \\leq \\beta_W $ and $ \\| diag(f'(h_{j-1}) \\| \\leq \\beta_h $\n",
    "    * We cannot figure out the dependency between long time interval's data\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-d63725db196675d327f3e4578c48701b\" width=\"500\">\n",
    "\n",
    "- How to fix vanishing gradients?\n",
    "    * Partial fix for gradient exploding: if ||g|| > threshold, shrink value of g\n",
    "    * Initialize W to be identity\n",
    "    * Use ReLU as activation function f\n",
    "- Main Idea of LSTM\n",
    "    * **Forget Gate** (\\*): how much old memory we want to keep; element-wise multiplication with old memory $ C_{t-1} $. The Parameters are learned as $ W_f $. I.e., $ \\sigma(W_f([h_{t-1}, X_t]) + b_f = f_t $. If you want all old memory, then $ f_t $ equals 1. After getting $ f_t $, multiply it with $ C_{t-1} $<br/><br/>\n",
    "   \n",
    "    * **New Memory Gate**(\\+)\n",
    "        * How to merge new memory with old memory; piece-wise summation, decides how to combine *new* memory with *old* memory. The weighing parameters are learned as $ W_i $. I.e., $ \\sigma(W_i([h_{t-1}, X_t]) + b_i = i_t $. \n",
    "    \n",
    "        * What is the new memory itself: $ tanh(W_C([h_{t-1}, X_t]) + b_C = \\tilde{C_t} $\n",
    "    \n",
    "        * What is the combined memory: $ C_{t-1} * f_t + \\tilde{C_t} * i_t = C_t$\n",
    "    \n",
    "    * \n",
    "    * **Output gate**: how much of the new memory we want to output or store? learned solely through combined memory. $ \\sigma(W_o([h_{t-1}, X_t]) + b_o = o_t $. Then the final output $ h_t $ would be $ o_t * tanh(C_t) = h_t $\n",
    "    \n",
    "    \n",
    "    \n",
    "- Why LSTM prevents gradient vanishing?\n",
    "    - *Linear* Connection between $C_t$ and $C_{t-1}$ rather than multiplying\n",
    "    - Forget gate controls and keeps long-distance dependency\n",
    "    - Allows error to flow at different strength based on inputs\n",
    "    - During initialization: Initialize forget gate bias to one: default to remembering\n",
    "    - See proof: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*laH0_xXEkFE0lKJu54gkFQ.png\" width=\"500\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.\" width=\"500\">\n",
    "\n",
    "\n",
    "- Other variation: Gated Recurrent Unit (GRU)\n",
    "    * **Update Gate**: How to combine old and new state: $ \\sigma(W_z([h_{t-1}, X_t])  = z_t $\n",
    "    * **Reset Gate**: How much to keep old state: $ \\sigma(W_r([h_{t-1}, X_t])= r_t $\n",
    "    * **New State**: $ tanh(WX_t + r_t * U h_{t-1}) =\\tilde{h_t}$ \n",
    "    * **Combine States**: $z_t* h_{t-1} + (1-z_t) * \\tilde{h_t} $\n",
    "    * If r=0, ignore/drop previos state for generating new state\n",
    "    * if z=1, carry information from past through many steps (long-term dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional LSTM\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/char_representation.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "1. From image: [CONV-POOL] \\* n --> FC Layer --> (num_example, 4096) written as **v**\n",
    "2. Hiddern layer: $ h = tanh(W_{xh} * X + W_{hh} * h + W_{ih} * \\bf{v} )$\n",
    "3. Output layer: $ y = W_{hy} * h\\ $\n",
    "4. Get input $ X_{t+1}\\ by\\ sampling\\ \\bf{y} $\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning with Attention\n",
    "\n",
    "To be filled after LSTM, TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
