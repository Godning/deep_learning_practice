{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Workflow\n",
    "- Preprocess Data (zero-centered,i.e.,Substractmean)\n",
    "- Identify architecture\n",
    "- Ensure that we can overfit a small traing set to acc = 100%\n",
    "- Loss not decreasing: too low leaning rate\n",
    "- Loss goes to NaN: too high learning rate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Improve gradient flow\n",
    "- Allow higher learning rates\n",
    "- Reduce dependence on initialization\n",
    "- Some regularization\n",
    "- *Note*: At test time, the mean from training should be used instead of calculated from testing batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- Fully connected layer\n",
    "    * X: (num_example, dimension)\n",
    "    * mu, sigma: 1 \\* D\n",
    "\n",
    "\n",
    "- CNN -> Spatial Batchnorm\n",
    "    * X: (num_example, channels, height, width)\n",
    "    * mu, sigma: 1 \\* C \\* 1 \\* 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization:\n",
    "- Fully connected layer\n",
    "    * X: (num_example, dimension)\n",
    "    * mu, sigma: N \\* 1 \n",
    "    * Note: same behavior during testing\n",
    "\n",
    "## Instance Normalization\n",
    "- CNN\n",
    "    * X: (num_example, channels, height, width)\n",
    "    * mu, sigma: N \\* C \\* 1 \\* 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Order Optimization\n",
    "- No Hyperparameter and learning rates\n",
    "- N^2 elements, O(N^3) for taking inverting\n",
    "- Methods:\n",
    "    * Quasi-Newton methods(BGFS): O(N^3) -> O(N^2)\n",
    "    * L-BFGS: Does not form/store the full inverse Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware\n",
    "- CPU: less cores, faster per core, better at sequential tasks\n",
    "- GPU: more cores, slower per core, better at parallel tasks\n",
    "- TPU: just for DL (Tensor Processing Unit)\n",
    "    * Split *One* graph over *Multiple* machines\n",
    "\n",
    "## Software\n",
    "- Caffe (FB)\n",
    "- PyTorch (FB)\n",
    "- TF (Google)\n",
    "- CNTK (MS)\n",
    "- Dynamic (e.g., Eager Execution) vs. Static (e.g., TF Lower-level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different CNN architectures\n",
    "\n",
    "- LeNet-5 (CONV-POOL-CONV-POOL-FC-FC)\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/tLKYz.png\" width=\"800\">\n",
    "\n",
    "- AlexNet 8 layers (CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8)\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_1.jpg\" width=\"400\">\n",
    "\n",
    "- ZFNet (similar with AlexNet)\n",
    "    * Smaller kernel, more filters\n",
    "\n",
    "- VGGNet(*smaller* filter and *deeper* networks)\n",
    "    * 16-19 layers in VGG16Net\n",
    "    * Three 3 \\* 3 kernel with stride == One 7 \\* 7 kernel; Same *effective receptive field\n",
    "    * But: 1) deeper network and more non-linearity; 2) less parameters ( 3 \\* 3 \\* 3 vs. 7 \\* 7)\n",
    "<img src=\"http://josephpcohen.com/w/wp-content/uploads/Screen-Shot-2016-01-14-at-11.25.15-AM.png\" width=\"800\">\n",
    "\n",
    "- GoogLeNet\n",
    "    * Introduced *inception* Module (Parallel filter operations with multiple kernel size)\n",
    "    * Problem: Output size too big after filter concatenation\n",
    "    * The purpose of 1 \\* 1 convolutonal layer: \n",
    "        - Pooling layer keeps the same depth as input\n",
    "        - 1 \\* 1 layer keeps the same dimension of input, and reduces depth (for example: 64 \\* 56 \\* 56 after 32 1 \\* 1 con --> 32 \\* 56 \\* 56)\n",
    "        - Reduce total number of operations\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Bo_Zhao48/publication/312515254/figure/fig3/AS:489373281067012@1493687090916/nception-module-of-GoogLeNet-This-figure-is-from-the-original-paper-10.jpg\" width=\"400\">\n",
    "\n",
    "- ResNet\n",
    "    * Use network layers to fit a *Residual mapping* instead of directly fitting a desired underlying mapping\n",
    "    * Residual blocks are stacked\n",
    "    * Similar to GoogLeNet, can use *bottelneck* layer (1 \\* 1 conv layer) for downsampling and efficiency ++\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Antonio_Theophilo/publication/321347448/figure/fig2/AS:565869411815424@1511925189281/Bottleneck-Blocks-for-ResNet-50-left-identity-shortcut-right-projection-shortcut.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of RNN: Image Captioning\n",
    "1. From image: [CONV-POOL] \\* n --> FC Layer --> (num_example, 4096) written as **v**\n",
    "2. Hiddern layer: $ h = tanh(W_{xh} * X + W_{hh} * h + W_{ih} * \\bf{v} )$\n",
    "3. Output layer: $ y = W_{hy} * h\\ $\n",
    "4. Get input $ X_{t+1}\\ by\\ sampling\\ \\bf{y} $\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning with Attention\n",
    "\n",
    "To be filled after LSTM, TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "- What is the problem with RNN\n",
    "    * Gradient Vanishing/Exploding with Vanilla RNN\n",
    "    * Computing gradient of $ h_0 $ involved many multiplication of **W** and **tanh** activation\n",
    "    * Brief proof:\n",
    "    \n",
    "    $ \\frac{ \\partial E_t} { \\partial w}\n",
    "    = \\sum_{k=1}^{t}  \\frac{ \\partial E_t} { \\partial y_t}\n",
    "                      \\frac{ \\partial y_t} { \\partial h_t}\n",
    "                      \\frac{ \\partial h_t} { \\partial h_k}\n",
    "                      \\frac{ \\partial h_k} { \\partial w}$\n",
    "    Here:$ h_t = W_{hh} f(h_{t-1}) + W_{hx} X_t$              \n",
    "    $ \\frac{ \\partial h_t} { \\partial h_k}\n",
    "    = \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}} $\n",
    "    \n",
    "    $ \\| \\frac{ \\partial h_t} { \\partial h_k} \\|\n",
    "    \\leq (\\beta_W \\beta_h)^{t-k} $ $\\beta$ is upper bound for matrix norms\n",
    "    \n",
    "    $ \\| W^T_{hh} \\| \\leq \\beta_W $ and $ \\| diag(f'(h_{j-1}) \\| \\leq \\beta_h $\n",
    "    * We cannot figure out the dependency between long time interval's data\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-d63725db196675d327f3e4578c48701b\" width=\"500\">\n",
    "\n",
    "- How to fix vanishing gradients?\n",
    "    * Partial fix for gradient exploding: if ||g|| > threshold, shrink value of g\n",
    "    * Initialize W to be identity\n",
    "    * Use ReLU as activation function f\n",
    "- Main Idea of LSTM\n",
    "    * **Forget Gate** (\\*): how much old memory we want to keep; element-wise multiplication with old memory $ C_{t-1} $. The Parameters are learned as $ W_f $. I.e., $ \\sigma(W_f([h_{t-1}, X_t]) + b_f = f_t $. If you want all old memory, then $ f_t $ equals 1. After getting $ f_t $, multiply it with $ C_{t-1} $<br/><br/>\n",
    "   \n",
    "    * **New Memory Gate**(\\+)\n",
    "        * How to merge new memory with old memory; piece-wise summation, decides how to combine *new* memory with *old* memory. The weighing parameters are learned as $ W_i $. I.e., $ \\sigma(W_i([h_{t-1}, X_t]) + b_i = i_t $. \n",
    "    \n",
    "        * What is the new memory itself: $ tanh(W_C([h_{t-1}, X_t]) + b_C = \\tilde{C_t} $\n",
    "    \n",
    "        * What is the combined memory: $ C_{t-1} * f_t + \\tilde{C_t} * i_t = C_t$\n",
    "    \n",
    "    * \n",
    "    * **Output gate**: how much of the new memory we want to output or store? learned solely through combined memory. $ \\sigma(W_o([h_{t-1}, X_t]) + b_o = o_t $. Then the final output $ h_t $ would be $ o_t * tanh(C_t) = h_t $\n",
    "    \n",
    "    \n",
    "    \n",
    "- Why LSTM prevents gradient vanishing?\n",
    "    - *Linear* Connection between $C_t$ and $C_{t-1}$ rather than multiplying\n",
    "    - Forget gate controls and keeps long-distance dependency\n",
    "    - Allows error to flow at different strength based on inputs\n",
    "    - During initialization: Initialize forget gate bias to one: default to remembering\n",
    "    - See proof: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*laH0_xXEkFE0lKJu54gkFQ.png\" width=\"500\">\n",
    "\n",
    "- Other variation: Gated Recurrent Unit (GRU)\n",
    "    * **Update Gate**: How to combine old and new state: $ \\sigma(W_z([h_{t-1}, X_t])  = z_t $\n",
    "    * **Reset Gate**: How much to keep old state: $ \\sigma(W_r([h_{t-1}, X_t])= r_t $\n",
    "    * **New State**: $ tanh(WX_t + r_t * U h_{t-1}) =\\tilde{h_t}$ \n",
    "    * **Combine States**: $z_t* h_{t-1} + (1-z_t) * \\tilde{h_t} $\n",
    "    * If r=0, ignore/drop previos state for generating new state\n",
    "    * if z=1, carry information from past through many steps (long-term dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation as RNN\n",
    "\n",
    "## Problem definition\n",
    "- Neural Machine Translation (NMT)\n",
    "- Sequence-to-Sequence(seq2seq) architecture\n",
    "- Difference from SMT (Statistical MT): calculate P(y|x) directly instead of using Bayes\n",
    "- Advantage: Single NN, less human engineering\n",
    "- Disadvantage: less interpretable, less control\n",
    "- Figure (TBA)\n",
    "\n",
    "## Main Components\n",
    "- Encoder RNN: encode source sentence, generate hidden state\n",
    "- Decoder RNN: **Language Model**, generate target sentence using outputs from encoder RNN; predict next word in *y* conditional on input *x*\n",
    "\n",
    "## Beam Search\n",
    "- Greedy decoding problem\n",
    "    * Instead of generating argmax each step, use beam search.\n",
    "    * Keep *k* most probable translations\n",
    "    \n",
    "## Attention model\n",
    "1. Get hidden states: $ h_1, ..., h_N $\n",
    "1. Get decoder state: $ s_t $\n",
    "1. Get attention scores by dot product: \n",
    "$ \\mathbf e^t = [s^T_t h_1, ..., s^T_t h_N] $\n",
    "1. Take softmax of $ \\mathbf e^t $ and get $ \\alpha_t $ which sum up to one\n",
    "1. Take weighted sum of hidder states basded on **h** and $\\mathbf\\alpha$ and get **a**\n",
    "1. Concatenate **a** and **s** in the decoder RNN\n",
    "\n",
    "Advantages:\n",
    "- Focus on certain parts of source\n",
    "- Provides shortcut / Bypass bottleneck\n",
    "- Get some interpretable results and learn alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Embedding s/ Word2vec\n",
    "\n",
    "- Why ther Options not working\n",
    "    * One-hot vectors (vocabulary list too big; No similarity measurement; how about new words)\n",
    "    * Co-currence vector (matrix given a certain window size, # of times two words are together)->Sparsity\n",
    "    * Singular Vector Decomposition (SVD) for cocurrence matrix (too expensive)\n",
    "    * Use a word's context to represent --> Word embedding\n",
    "    \n",
    "    \n",
    "    \n",
    "- Key Components\n",
    "    * Center word *c*, context word *o*\n",
    "    * Two vectors for each word *w*: $ v_w $ and $ u_w $. $\\theta$ contains all *u* and *v* (Input and Output Vector)\n",
    "    * For example: $ P( w1|w2 ) = P(w_2|w_1;  u_{w2}, v_{w1}, \\theta )$\n",
    "    * Calculate u*v for each word, and use softmax to derive probability\n",
    "    * After optimization for loss, get two vectors for each word. Combine or Use *u* or Use *v*\n",
    "    \n",
    "    \n",
    "    \n",
    "- Variation\n",
    "    * Skip-grams (SG):given center, predict context\n",
    "    * Countinous Bag of Words (CBOW):given bag of context, predict center\n",
    "    * Negative sampling (maximize p of actual context + minimize p of random context i.e. noise)\n",
    "    * GloVe: combine count-based and direct-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Window Classification\n",
    "- Difference with typical ML: learn both **W** and word vectors **x**\n",
    "- Task definition: classify a word in its *Context Window*\n",
    "    * Do not train single word: ambiguity\n",
    "    * Do not just average over window: lose position information\n",
    "    * Get a vector X with length of 5d where 5 is window size and d is embedding size\n",
    "    * Predict y based on softmax of WX and minimize cross-entropy error\n",
    "    \n",
    "    \n",
    "    \n",
    "- What happens for x:\n",
    "    * Updated just as weigh W\n",
    "    * Pushed into an area helpful for classification task\n",
    "    * Example: $X_{in}$ may be a sign for location (One of the *Named Entity Recognition* tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Modeling\n",
    "- Task Definition: Predict next word\n",
    "- Approach 1: N-gram model\n",
    "    * Using count of different length of grams as they shown in corpus\n",
    "    * Main problem: *Sparsity*\n",
    "  \n",
    "\n",
    "- Approach 2: NN Model with Fixed Window Size\n",
    "    * No Sparsity problem\n",
    "    * Model size reduced\n",
    "    * BUT: X do not share weight, and how to decide window size\n",
    "\n",
    "\n",
    "- Approach 3: RNN Model\n",
    "    * Any  sequence length will work\n",
    "    * Weights shared, model size doesn't increase\n",
    "    * BUT: computation is slow (why) and cannot access information from many steps back\n",
    "\n",
    "\n",
    "- Application\n",
    "    * One-to-one: tagging each word\n",
    "    * many-to-one: sentiment analysis\n",
    "    * Encoder module: example: element-wise max of all hidden states -> as input for further NN model\n",
    "    \n",
    "    \n",
    "    \n",
    "- Alternative tasks in NLP - *Conditioned* Language Models\n",
    "    * Speech recognition\n",
    "    * Machine translation\n",
    "    * Generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
