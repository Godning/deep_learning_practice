{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Neural Network from Scratch using Numpy**\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Some key concepts***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "- Batch (Fast Convergence)\n",
    "- SGD\n",
    "\n",
    "**Back-propagation**\n",
    "- Preprocessing (e.g., zero-centered)\n",
    "\n",
    "**Activation Function**\n",
    "- Sigmoid\n",
    "    * Saturated at 0/1 and kills gradients (derivative -> 0)\n",
    "    * Output not zero-centred; for next layer: f = wx + b, x>0, df/dw same sign for all w; zig-zag update trajectory\n",
    "- Tanh\n",
    "    * Still kills gradients\n",
    "    * But: zero-cented\n",
    "- ReLU\n",
    "    * Non-saturated, linearity --> Accelerate convergence\n",
    "    * Cheap computation\n",
    "    * But: Can die; never activate\n",
    "    * Extension: Leaky ReLU, maxout\n",
    "    \n",
    "**NN as universal approximators**\n",
    "- More neurons --> more complicated functions\n",
    "- Regularization to prevent overfitting\n",
    "\n",
    "**Regularization**\n",
    "- L1/L2/ElasticNet\n",
    "- Max Norm constraint\n",
    "- Drop Out layer\n",
    "\n",
    "**Hyperparameter Optimization**\n",
    "- Single validation set > cross validation in practice\n",
    "- Random search instead of grid search within a range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvNets**\n",
    "- Difference with regular NN:\n",
    "    * Main difference: each neuron is layer 2 is only connected to a few neurons in layer 1\n",
    "    * Data arranged in 3 dimensions: height, width, and depth\n",
    "- Convolutional Layer:\n",
    "    * Filter (with full depth, but local connectivity across 2d), 3\\*3 --> 5\\*5\n",
    "    * The depth of layer 2 == The number of filters in Layer 1\n",
    "    * `Stride`: usually 1, leaving downsampling to pooling layer. Can use 2 to compromise 1st layer because of computational constraints\n",
    "    * `Padding`: use same to avoid missing information along the border\n",
    "    * *Parameter Sharing*: Same weight for filter/kernel at same depth slice in layer 2 (Alternative: local)\n",
    "- Pooling\n",
    "    * Most commonly: 2-2 Max Pooling\n",
    "- Fully-Connected Layer\n",
    "- Common architecture:\n",
    "    * INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]\\* -> [FC -> RELU]\\*2 -> FC\n",
    "    * Prefer a stack of small filter CONV to one large receptive field CONV layer\n",
    "- Challenge: Computational resources\n",
    "\n",
    "\n",
    "**Transfer Learning**\n",
    "- Apply trained model without last FC layer and use it as feature extracter\n",
    "- Continue to fine tune the model using smaller learning rate\n",
    "- Can use different image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gates (Add, Multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notations:\n",
    "$$\n",
    "WX + b = \\mathbf{S} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{WX}} = d(WX) \\\\\n",
    "\\frac{\\partial{L}}{\\partial{S}} = d({S}) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math: http://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Layers.py\n",
    "import numpy as np\n",
    "from script.Optimization import *\n",
    "class Mul:\n",
    "    def forward(self, W, X):\n",
    "        return np.dot(X, W)\n",
    "    \n",
    "    def backward(self, W, X, dWX):\n",
    "        dW = np.dot( np.transpose(X), dWX )\n",
    "        dX = np.dot( dWX, np.transpose (W))\n",
    "        return dW, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class Add:\n",
    "    def forward(self, WX, b):\n",
    "        return WX + b\n",
    "\n",
    "    def backward(self, WX, b, dS):\n",
    "        dWX = dS * np.ones_like(WX, dtype=np.float64)\n",
    "        db = np.dot(np.ones((1, dS.shape[0]), dtype=np.float64), dS)\n",
    "        return db, dWX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layers (ReLU, Sigmoid, Tanh, Softmax, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Activation(S) = Z \\\\\n",
    "Input: \n",
    "\\frac{\\partial{L}}{\\partial{Z}} = d(Z) \\\\\n",
    "Output:\n",
    "\\frac{\\partial{L}}{\\partial{S}} = d(S) \\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, S):\n",
    "        Z = S * (S > 0)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        dS = 1. * (S > 0) * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, S):\n",
    "        Z = np.tanh(S)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        Z = self.forward(S)\n",
    "        dS = (1.0 - np.square(Z)) * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, S):\n",
    "        Z = 1. / (1.0 + np.exp(-S))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        Z = self.forward(S)\n",
    "        dS =(1 - Z) * Z * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class  Softmax:\n",
    "    # For Training\n",
    "    def __init__(self):\n",
    "        self.num_examples = 0\n",
    "    \n",
    "    def forward(self, S):\n",
    "        self.num_examples = S.shape[0]\n",
    "        exp_S = np.exp(S)\n",
    "        Z = exp_S / np.sum(exp_S, axis = 1, keepdims = True)\n",
    "        return Z\n",
    "\n",
    "    def backward(self, S, y): # Note: y instead of dZ\n",
    "        probs = Z = self.forward(S)\n",
    "        for i in range(len(y)):\n",
    "            true_label = y[i]\n",
    "            probs[i][true_label] -= 1 # see equation above\n",
    "        dS = probs\n",
    "        return dS\n",
    "    \n",
    "    # For evaluation    \n",
    "    def forward_loss(self, Z, y):\n",
    "        probs = Z\n",
    "        log_probs = []\n",
    "        for prob, true_label in zip(probs, y):\n",
    "            log_probs.append(np.log(prob[true_label]))\n",
    "        avg_cross_entropy_loss = - 1. / self.num_examples * np.sum(log_probs) # see equation above\n",
    "        return avg_cross_entropy_loss\n",
    "    \n",
    "    # For prediction\n",
    "    def predict(self, Z):\n",
    "        return np.argmax(Z, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "for\\ each\\ sample\\ i: \n",
    "$\n",
    "$$\n",
    "\\hat{y_{k}} = softmax(S_1, S_2, ..., S_{k}),\\ k\\ is\\ class\\ index \\\\\n",
    "\\mathbf{E} = - \\sum_{i=1}^N y_{ik} log(\\hat{y_{ik}} )\\\\\n",
    "\\frac{\\partial\\mathbf{E}}{\\partial S_{k}} = {\\hat y}_{k} - y_{k},\\ where\\ y_{k} = 0, 1\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- **Idea**: \n",
    "    - Normalize the inputs before activation function / after w*x + b\n",
    "    - Differentiable operation\n",
    "    - Robust to bad initialization\n",
    "- **Advantages**: \n",
    "    - Faster training;\n",
    "    - Allow scale and shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self):\n",
    "        self.cache = ()\n",
    "        \n",
    "    def forward(self, X, gamma, beta, eps):\n",
    "        num_examples = X.shape[0]\n",
    "        \n",
    "        mu_B = 1. / num_examples * np.sum(X, axis = 0)\n",
    "        X_mu = X - mu_B\n",
    "        var_B = 1. / num_examples * np.sum(  X_mu ** 2, axis = 0 )\n",
    "        sqrt_var_B = np.sqrt(var_B + eps)\n",
    "        i_sqrt_var_B = 1. / sqrt_var_B\n",
    "        X_hat =  X_mu * i_sqrt_var_B\n",
    "        gammaX = gamma * X_hat\n",
    "        DZ = gammaX + beta\n",
    "        \n",
    "        self.cache = (X_hat, X_mu, gamma, i_sqrt_var_B, sqrt_var_B, var_B, eps)\n",
    "        return DZ\n",
    "    \n",
    "    def backward(self, dDZ):\n",
    "        num_examples = dDZ.shape[0]\n",
    "        X_hat, X_mu, gamma, i_sqrt_var_B, sqrt_var_B, var_B, eps = self.cache\n",
    "        \n",
    "        # scale and shift\n",
    "        dbeta = np.sum(dDZ, axis = 0)\n",
    "        dgammaX = dDZ\n",
    "        dgamma = np.sum(dgammaX * X_hat, axis = 0)\n",
    "        dXhat = dgammaX * gamma\n",
    "        \n",
    "        # Standardize\n",
    "        di_sqrt_var_B = np.sum(dXhat * X_mu, axis = 0)\n",
    "        d_x_mu_2 = dXhat * i_sqrt_var_B\n",
    "        dsqrt_var_B = -1. / (sqrt_var_B ** 2) * di_sqrt_var_B\n",
    "        dvar_B = 0.5 * 1. / np.sqrt(var_B + eps) * dsqrt_var_B\n",
    "\n",
    "        # Batch variance\n",
    "        dsquare = 1. / num_examples * np.ones_like(dDZ) * dvar_B\n",
    "        d_x_mu_1 = 2 * X_mu * dsquare\n",
    "        \n",
    "        # Batch mean\n",
    "        d_x_mu = d_x_mu_2 + d_x_mu_1 # d(f(x)g(x)) = f(x)g'(x) = f'(x)g(x)\n",
    "        dmu = -1. * np.sum(d_x_mu, axis = 0)\n",
    "        dx_2 = d_x_mu\n",
    "        dx_1 = 1. / num_examples * np.ones_like(dDZ) * dmu\n",
    "        dx = dx_2 + dx_1\n",
    "        \n",
    "        return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util.im2col import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/wiseodd/hipsternet/blob/master/hipsternet/layer.py\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.cache = ()\n",
    "        \n",
    "    def forward(self, X, W, b, stride=1, padding=1):\n",
    "        cache = W, b, stride, padding\n",
    "        n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "        n_x, d_x, h_x, w_x = X.shape\n",
    "        h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "        w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "        \n",
    "        if not h_out.is_integer() or not w_out.is_integer():\n",
    "            raise Exception('Invalid output dimension!')\n",
    "\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "        X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "        W_col = W.reshape(n_filters, -1)\n",
    "        \n",
    "        out = W_col @ X_col + b\n",
    "        out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "        self.cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X, W, b, stride, padding, X_col = self.cache\n",
    "        n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "        db = np.sum(dout, axis=(0, 2, 3))\n",
    "        db = db.reshape(n_filter, -1)\n",
    "\n",
    "        dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "        dW = dout_reshaped @ X_col.T\n",
    "        dW = dW.reshape(W.shape)\n",
    "\n",
    "        W_reshape = W.reshape(n_filter, -1)\n",
    "        dX_col = W_reshape.T @ dout_reshaped\n",
    "        dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "        return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "# Default padding: SAME\n",
    "NUM_EXAMPLES = 2\n",
    "NUM_CHANNELS = 3\n",
    "NUM_FILTERS = 3\n",
    "INPUT_HEIGHT = 4\n",
    "INPUT_WIDTH =  4\n",
    "FILTER_HEIGHT = 4\n",
    "FILTER_WIDTH = 4\n",
    "STRIDE = 2\n",
    "PAD = 1\n",
    " \n",
    "x_shape = (NUM_EXAMPLES, NUM_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH) # num_examples, depth(channels), height, width\n",
    "w_shape = (NUM_FILTERS, NUM_CHANNELS, FILTER_HEIGHT, FILTER_WIDTH) # num_filters, depth(channels), kernel_height, kernel_width\n",
    "b_shape = (NUM_FILTERS, 1)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=np.prod(b_shape)).reshape(b_shape)\n",
    "\n",
    "cnn = CNN()\n",
    "out = cnn.forward(x, w, b, STRIDE, PAD)\n",
    "dout = np.ones_like(out)\n",
    "dX, dW, db = cnn.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def forward(self, X, size = 2, stride = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_forward(X, size=2, stride=2):\n",
    "    def maxpool(X_col):\n",
    "        max_idx = np.argmax(X_col, axis=0)\n",
    "        out = X_col[max_idx, range(max_idx.size)]\n",
    "        return out, max_idx\n",
    "\n",
    "    return _pool_forward(X, maxpool, size, stride)\n",
    "\n",
    "\n",
    "def maxpool_backward(dout, cache):\n",
    "    def dmaxpool(dX_col, dout_col, pool_cache):\n",
    "        dX_col[pool_cache, range(dout_col.size)] = dout_col\n",
    "        return dX_col\n",
    "\n",
    "    return _pool_backward(dout, dmaxpool, cache)\n",
    "\n",
    "def _pool_forward(X, pool_fun, size=2, stride=2):\n",
    "    n, d, h, w = X.shape\n",
    "    h_out = (h - size) / stride + 1\n",
    "    w_out = (w - size) / stride + 1\n",
    "\n",
    "    if not w_out.is_integer() or not h_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_reshaped = X.reshape(n * d, 1, h, w)\n",
    "    X_col = im2col_indices(X_reshaped, size, size, padding=0, stride=stride)\n",
    "\n",
    "    out, pool_cache = pool_fun(X_col)\n",
    "\n",
    "    out = out.reshape(h_out, w_out, n, d)\n",
    "    out = out.transpose(2, 3, 0, 1)\n",
    "\n",
    "    cache = (X, size, stride, X_col, pool_cache)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def _pool_backward(dout, dpool_fun, cache):\n",
    "    X, size, stride, X_col, pool_cache = cache\n",
    "    n, d, w, h = X.shape\n",
    "\n",
    "    dX_col = np.zeros_like(X_col)\n",
    "    dout_col = dout.transpose(2, 3, 0, 1).ravel()\n",
    "\n",
    "    dX = dpool_fun(dX_col, dout_col, pool_cache)\n",
    "\n",
    "    dX = col2im_indices(dX_col, (n * d, 1, h, w), size, size, padding=0, stride=stride)\n",
    "    dX = dX.reshape(X.shape)\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 1: Weights Initialization\n",
    "- **All zero**: wrong: neuron outputs and gradients would be same; same update\n",
    "- **Number to small**: small gradients for inputs; gradient diminishing when flowing backwafrd\n",
    "- **Preferred: All neuron with same output distribution**: \n",
    "    - w = np.random.randn(n) / sqrt(n), where n is number of inputs. \n",
    "    - It can be proved that Var(S) = Var(WX) = Var(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 2: Param Update and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step decay for learning rate: \n",
    "    * Reduce the learning rate by some factor every few epochs. \n",
    "    * Other approaches also avalable, like exponential decay, 1/t decay, etc.\n",
    "- Second-order update method:\n",
    "    * i.e., Newton's method, not common\n",
    "- Per-parameter adaptive learning rate methods: \n",
    "    * For example: Adagrad, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Optimization.py\n",
    "import numpy as np\n",
    "# Treat all elements of dX as a whole\n",
    "#  Intuition: \n",
    "#  If gradient direction not changed, increase update, faster convergence\n",
    "#  If gradient direction changed, reduce update, reduce oscillation\n",
    "\n",
    "def VanillaUpdate(x, dx, learning_rate):\n",
    "    x += -learning_rate * dx\n",
    "    return x\n",
    "\n",
    "def MomentumUpdate(x, dx, v, learning_rate, mu):\n",
    "    v = mu * v - learning_rate * dx # integrate velocity, mu's typical value is about 0.9\n",
    "    x += v # integrate position     \n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Optimization.py\n",
    "\n",
    "# Treat each element of dX adaptively\n",
    "# Intuition:\n",
    "# 1. Those dx receiving infrequent updates should have higher learning rate. vice versa \n",
    "# 2. We don't want: the gradients accumulate, and the learning rate monotically decrease, \n",
    "# 2. We want: modulates the learning rate of each weight based on the magnitudes of its gradient\n",
    "# 3. Still want to use \"momentum-like\" update to get a smooth gradient\n",
    "\n",
    "# 1. AdaGrad\n",
    "def AdaGrad(x, dx, learning_rate, cache, eps):\n",
    "    cache += dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps) # (usually set somewhere in range from 1e-4 to 1e-8)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2. RMSprop\n",
    "def RMSprop(x, dx, learning_rate, cache, eps, decay_rate): #Here, decay_rate typical values are [0.9, 0.99, 0.999]\n",
    "    cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2+3. Adam\n",
    "def Adam(x, dx, learning_rate, m, v, t, beta1, beta2, eps):\n",
    "    m = beta1*m + (1-beta1)*dx # Smooth gradient\n",
    "    #mt = m / (1-beta1**t) # bias-correction step\n",
    "    v = beta2*v + (1-beta2)*(dx**2) # keep track of past updates\n",
    "    #vt = v / (1-beta2**t) # bias-correction step\n",
    "    x += - learning_rate * m / (np.sqrt(v) + eps) # eps = 1e-8, beta1 = 0.9, beta2 = 0.999   \n",
    "    return x, m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Optimization.py\n",
    "\n",
    "class WeightUpdate:\n",
    "    def __init__(self, init_value):\n",
    "        self.val = init_value\n",
    "        self.cache = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.m = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.v = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.t = 0\n",
    "    \n",
    "    def Update(self, d, learning_rate, lambda_ , method):\n",
    "        \n",
    "        old_val = self.val\n",
    "        if method == 'Vanilla':\n",
    "            self.val = VanillaUpdate(self.val, d, learning_rate)\n",
    "        elif method == 'MomentumUpdate':\n",
    "            self.val, self.v = MomentumUpdate(self.val, d, self.v, learning_rate, mu = 0.9)\n",
    "        elif method == 'AdaGrad':\n",
    "            self.val, self.cache = AdaGrad(self.val, d, learning_rate, self.cache, eps = 1e-5)\n",
    "        elif method == 'RMSprop':\n",
    "            self.val, self.cache = AdaGrad(self.val, d, learning_rate, self.cache, eps = 1e-5, decay_rate = 0.99)\n",
    "        elif method == 'Adam':\n",
    "            self.val, self.m, self.v = Adam(self.val, d, learning_rate, self.m, self.v, self.t, beta1 = 0.9, beta2 = 0.999, eps = 1e-8)  \n",
    "            self.t += 1\n",
    "            \n",
    "        # Regularization\n",
    "        self.val -= lambda_ * old_val\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layers.py\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, activation_function, num_neurons, batch_norm = False, dropout_p = 1.0):\n",
    "        self.dim = num_neurons\n",
    "        self.activation = activation_function\n",
    "        self.batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.batchnorm = BatchNorm()\n",
    "        self.isfirst = False\n",
    "        self.islast = False\n",
    "        self.before = None\n",
    "        self.p = dropout_p\n",
    "\n",
    "    def set_first_layer(self, input):\n",
    "        self.isfirst = True\n",
    "        self.X = input\n",
    "        \n",
    "    def set_last_layer(self, y):\n",
    "        self.islast = True\n",
    "        self.y = y\n",
    "    \n",
    "    def initialize_Wb(self):\n",
    "        if self.isfirst:\n",
    "            before_dim = self.X.shape[1]\n",
    "        else:\n",
    "            before_dim = self.before.dim\n",
    "        self.W = np.random.randn(before_dim, self.dim) / np.sqrt(before_dim) # see notes above\n",
    "        self.b = np.random.randn(self.dim).reshape(1, self.dim) # see notes above\n",
    "        self.gamma, self.beta = (1., 0.)\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        if not self.isfirst:\n",
    "            self.X = self.before.Z\n",
    "            \n",
    "        self.mask = np.random.rand(*self.W.shape) < self.p / self.p\n",
    "        self.W *= self.mask\n",
    "        self.WX = Mul().forward( self.W, self.X )\n",
    "        self.S = Add().forward( self.WX, self.b)\n",
    "        if self.batch_norm:\n",
    "            self.SZ = self.batchnorm.forward( self.S, self.gamma, self.beta, eps = 0)\n",
    "        else:\n",
    "            self.SZ = self.S\n",
    "        self.Z = self.activation.forward(self.SZ)\n",
    "            \n",
    "    def backward_propagation(self):\n",
    "        if self.islast:\n",
    "            self.dZ = self.y\n",
    "        \n",
    "        self.dSZ = self.activation.backward(self.SZ, self.dZ)\n",
    "        if self.batch_norm:\n",
    "            self.dS, self.dgamma, self.dbeta = self.batchnorm.backward(self.dSZ)\n",
    "        else:\n",
    "            self.dS = self.dSZ\n",
    "            self.dgamma, self.dbeta = 0,0\n",
    "        self.db, self.dWX = Add().backward(self.WX, self.b, self.dS)\n",
    "        self.dW, self.dX = Mul().backward(self.W, self.X, self.dWX)\n",
    "        self.dW *= self.mask\n",
    "        \n",
    "        if not self.isfirst:\n",
    "            self.before.dZ = self.dX\n",
    "    \n",
    "    def update_weight(self, learning_rate, lambda_ , method):\n",
    "        \n",
    "        # Create variable list\n",
    "        self.weights = [self.W, self.b, self.gamma, self.beta]\n",
    "        self.ds = [self.dW, self.db, self.dgamma, self.dbeta]\n",
    "\n",
    "        # First Time\n",
    "        if not hasattr(self, 'updates'):      \n",
    "            self.updates = []\n",
    "            for weight in self.weights:\n",
    "                self.updates.append(WeightUpdate(weight))\n",
    "        \n",
    "        # Calculate update for each iteration\n",
    "        new_weights = []\n",
    "        for weight_update, d in zip(self.updates, self.ds):\n",
    "            new_weights.append(weight_update.Update(d, learning_rate, lambda_, method))\n",
    "        \n",
    "        # Update weights\n",
    "        self.W, self.b, self.gamma, self.beta = new_weights\n",
    "        \n",
    "        # Vanilla version\n",
    "        #self.W += -learning_rate * self.dW + (- lambda_ * self.W)\n",
    "        #self.b += -learning_rate * self.db\n",
    "        #self.gamma += -learning_rate * self.dgamma\n",
    "        #self.beta  += -learning_rate * self.dbeta\n",
    "            \n",
    "    # Only for softmax layer\n",
    "    def calculate_loss(self):\n",
    "        loss = self.activation.forward_loss(self.Z, self.y)\n",
    "        return loss\n",
    "            \n",
    "    def predict(self):\n",
    "        return self.activation.predict(self.Z)\n",
    "    \n",
    "    def calculate_acc(self): \n",
    "        pred = self.predict()\n",
    "        return sum( pred == self.y ) / len(self.y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Network.py\n",
    "import numpy as np\n",
    "from script.Layers import *\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.input = []\n",
    "        self.y = []\n",
    "        \n",
    "    def add(self, new_layer):\n",
    "        if self.layers:\n",
    "            new_layer.before = self.layers[-1]\n",
    "        self.layers.append(new_layer)\n",
    "    \n",
    "    def load_data(self, input, y):\n",
    "        self.layers[0].set_first_layer(input)\n",
    "        self.layers[-1].set_last_layer(y)\n",
    "        \n",
    "    def initialize(self, input, y, batch_size):\n",
    "        self.input = input\n",
    "        self.y = y\n",
    "        self.load_data(input[:batch_size,:], y[:batch_size])\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_Wb()\n",
    "\n",
    "    def train(self, num_iter, learning_rate, batch_size, rand_, lambda_, optimizer = 'Vanilla', Val_X = None, Val_y = None, \n",
    "             CAL_STEP = 100, PRINT_STEP = 100):\n",
    "        \n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        \n",
    "        for i in range(num_iter):    \n",
    "            # Calculate batch index\n",
    "            if not rand_:\n",
    "                idx = list(range(self.input.shape[0]))\n",
    "            else:\n",
    "                idx = np.random.randint(self.input.shape[0], size = batch_size)\n",
    "            \n",
    "            self.load_data(self.input[idx,:], self.y[idx])\n",
    "            \n",
    "            # Forward Propagation\n",
    "            for layer in self.layers:\n",
    "                layer.forward_propagation()\n",
    "                \n",
    "            # Print Traing Acc/Loss\n",
    "            if (i % CAL_STEP == 0):\n",
    "                t_loss = self.layers[-1].calculate_loss()\n",
    "                t_acc  = self.layers[-1].calculate_acc()\n",
    "                train_loss.append(t_loss)\n",
    "                train_acc.append(t_acc)\n",
    "                \n",
    "            if (i % PRINT_STEP == 0):\n",
    "                print('Train at Iter {0:2d}: loss - {1:.3f}, Acc - {2:.3f}'.format(i, t_loss, t_acc))\n",
    "                \n",
    "            # Backward Propagation\n",
    "            for layer in self.layers[::-1]:\n",
    "                layer.backward_propagation()\n",
    "                layer.update_weight(learning_rate, lambda_ = lambda_ , method = optimizer)\n",
    "            \n",
    "            # Print Validation Acc/Loss\n",
    "            if (i % CAL_STEP == 0 and Val_X is not None):\n",
    "                v_acc, v_loss = self.evaluate(Val_X, Val_y)\n",
    "                val_loss.append(v_loss)\n",
    "                val_acc.append(v_acc) \n",
    "                \n",
    "            if (i % PRINT_STEP == 0 and Val_X is not None):\n",
    "                print('Validation at Iter {0:2d}: loss - {1:.3f}, Acc - {2:.3f}'.format(i, v_loss, v_acc))\n",
    "\n",
    "        # Finally return loss list\n",
    "        return train_loss, train_acc, val_loss, val_acc\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.load_data(X, y = None)\n",
    "        for layer in self.layers:\n",
    "            layer.forward_propagation()\n",
    "        return layers[-1].predict()\n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        self.load_data(X, y)\n",
    "        for layer in self.layers:\n",
    "            layer.forward_propagation()\n",
    "        loss = self.layers[-1].calculate_loss()\n",
    "        acc  = self.layers[-1].calculate_acc()\n",
    "        return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
